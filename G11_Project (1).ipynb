{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "G11_Project.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "6014gBiWMpdc",
        "Jq4iFWHV1SYD",
        "dc1Dig-YjYd5",
        "8v7yitKbmL9G",
        "qKwBk7R-mhNh"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT1X3-0IMTxU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXJA1sUAjFMX"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6Qoxdqth_5T"
      },
      "source": [
        "!pip install snscrape\n",
        "!pip install wordcloud\n",
        "!pip3 install textblob\n",
        "!pip3 install imblearn\n",
        "\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import itertools\n",
        "import tweepy\n",
        "import csv\n",
        "import datetime\n",
        "import time\n",
        "import tweepy\n",
        "\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from collections import defaultdict\n",
        "from datetime import date\n",
        "import termcolor\n",
        "import sys\n",
        "from termcolor import colored, cprint\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "import re # for regular expressions\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv44Lo_-K1Hn"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "import re  # for regular expressions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLjFUymzW3-6"
      },
      "source": [
        "#Install package for Emotion Analysis\n",
        "!pip install text2emotion\n",
        "#Import the modules\n",
        "import text2emotion as te\n",
        "\n",
        "#Install package for NB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "# import method for split train/test data set\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import method to calculate metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Packages for LSI\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#imblearn packages for smote\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTEENN\n",
        "from collections import Counter\n",
        "\n",
        "# Import packages\n",
        "!pip install mglearn\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import mglearn\n",
        "import os\n",
        "import glob\n",
        "import pickle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzRPsBnPMW0V"
      },
      "source": [
        "# DATA COLLECTION \n",
        "\n",
        "We extract a total of 30000 tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7CW_siSjFv5"
      },
      "source": [
        "def collect_data(dates):\n",
        "    date_initial = [['2019-11-30', '2020-05-30',5000]]\n",
        "    for i in range(len(date_initial)):\n",
        "        print(date_initial[i][0],date_initial[i][1],date_initial[i][2])\n",
        "        df = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper('\"stress + since:\"'+ date_initial[i][0] +'\"until:\"'+ date_initial[i][1] + '\"lang:en\"').get_items(), date_initial[i][2]))\n",
        "    print(dates)\n",
        "    print(len(dates))\n",
        "    for i in range(len(dates)):\n",
        "        print(dates[i][0],dates[i][1],dates[i][2])\n",
        "        df=df.append(pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper('\"stress + since:\"'+ dates[i][0] +'\"until:\"'+ dates[i][1] + '\"lang:en\"').get_items(), dates[i][2])))\n",
        "        \n",
        "    print(df)    \n",
        "    df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/BIA/tweets104.csv\", index=False)\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbPaQz_WjLF5"
      },
      "source": [
        "date_list = [['2020-05-30','2020-11-30',10000],['2020-11-30','2021-05-30',10000],['2021-05-30','2021-10-15',5000]]\n",
        "collect_data(date_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlprLUZEYTn8"
      },
      "source": [
        "#Drop unwanted columns\n",
        "df = df.drop(['tcooutlinksss','outlinksss', 'outlinks', 'tcooutlinks'],axis=1)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq2Sz0lRO7hC"
      },
      "source": [
        "read_file = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/BIA/tweets103.csv\")\n",
        "read_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9_OGGU8a05X"
      },
      "source": [
        "# TEXT PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IuIMg63a3wr"
      },
      "source": [
        "\n",
        "from bs4 import BeautifulSoup\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#remove duplicated values\n",
        "read_file.drop_duplicates(inplace=True) \n",
        "\n",
        "\n",
        "def load_dict_smileys():\n",
        "\n",
        "    return {\n",
        "        \":‑)\":\"\",\n",
        "        \":-]\":\"\",#\"smiley\",\n",
        "        \":-3\":\"\",#\"smiley\",\n",
        "        \":->\":\"\",#\"smiley\",\n",
        "        \"8-)\":\"\",#\"smiley\",\n",
        "        \":-}\":\"\",#\"smiley\",\n",
        "        \":)\":\"\",#\"smiley\",\n",
        "        \":]\":\"\",#\"smiley\",\n",
        "        \":3\":\"\",#\"smiley\",\n",
        "        \":>\":\"\",#\"smiley\",\n",
        "        \"8)\":\"\",#\"smiley\",\n",
        "        \":}\":\"\",#\"smiley\",\n",
        "        \":o)\":\"\",#\"smiley\",\n",
        "        }\n",
        "\n",
        "def load_dict_contractions():\n",
        "\n",
        "    return {\n",
        "        \"ain't\":\"is not\",\n",
        "        \"amn't\":\"am not\",\n",
        "        \"aren't\":\"are not\",\n",
        "        \"can't\":\"cannot\",\n",
        "        \"'cause\":\"because\",\n",
        "        \"couldn't\":\"could not\",\n",
        "        \"couldn't've\":\"could not have\",\n",
        "        \"could've\":\"could have\",\n",
        "        \"daren't\":\"dare not\",\n",
        "        \"daresn't\":\"dare not\",\n",
        "        }\n",
        "\n",
        "# the above said preprocessing functions\n",
        "##\n",
        "def remove_url(text):\n",
        "  return re.sub(r'https?://\\S+|www\\.\\S+', repl=r'', string=text)\n",
        "\n",
        "\n",
        "def remove_numbers(text):\n",
        "  return ''.join([c for c in text if not c.isdigit()])\n",
        "\n",
        "\n",
        "def remove_punct(text):\n",
        "  return ''.join([c for c in text if c not in string.punctuation])\n",
        "\n",
        "\n",
        "def remove_stopword(text):\n",
        "  return ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
        "\n",
        "\n",
        "def correct_spelling(text):\n",
        "  return str(TextBlob(str(text)).correct())\n",
        "\n",
        "\n",
        "def do_lemmatize(text):\n",
        "  return ' '.join([lemmatizer.lemmatize(str(word), \"v\") for word in text.split()])\n",
        "\n",
        "\n",
        "def html (text): \n",
        "    #Escaping HTML characters\n",
        "    tweet = BeautifulSoup(text, \"lxml\").get_text()\n",
        "    return tweet\n",
        "\n",
        "\n",
        "def remove_mention (text): \n",
        "    # Removing the @\n",
        "    return re.sub(r\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", '', string=text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYiAkGombVqg"
      },
      "source": [
        "\n",
        "EMOTICONS = {\n",
        "    u\":‑\\)\":\"Happy face or smiley\",\n",
        "    u\":\\)\":\"Happy face or smiley\",\n",
        "    u\":-\\]\":\"Happy face or smiley\",\n",
        "    u\":\\]\":\"Happy face or smiley\",\n",
        "    u\":-3\":\"Happy face smiley\",\n",
        "    u\":3\":\"Happy face smiley\",\n",
        "    u\":->\":\"Happy face smiley\",\n",
        "    u\":>\":\"Happy face smiley\",\n",
        "    u\"8-\\)\":\"Happy face smiley\",\n",
        "    u\":o\\)\":\"Happy face smiley\",\n",
        "    u\":-\\}\":\"Happy face smiley\",\n",
        "    u\":\\}\":\"Happy face smiley\",\n",
        "    u\":-\\)\":\"Happy face smiley\",\n",
        "    u\":c\\)\":\"Happy face smiley\",\n",
        "    u\":\\^\\)\":\"Happy face smiley\",\n",
        "    u\"=\\]\":\"Happy face smiley\",\n",
        "    u\"=\\)\":\"Happy face smiley\",\n",
        "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":-\\)\\)\":\"Very happy\",\n",
        "    u\":‑\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑c\":\"Frown, sad, andry or pouting\",\n",
        "    u\":c\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‑\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
        "    u\":@\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":'‑\\(\":\"Crying\",\n",
        "    u\":'\\(\":\"Crying\",\n",
        "    u\":'‑\\)\":\"Tears of happiness\",\n",
        "    u\":'\\)\":\"Tears of happiness\",\n",
        "    u\"D‑':\":\"Horror\",\n",
        "    u\"D:<\":\"Disgust\",\n",
        "    u\"D:\":\"Sadness\",\n",
        "    u\"D8\":\"Great dismay\",\n",
        "    u\"D;\":\"Great dismay\",\n",
        "    u\"D=\":\"Great dismay\",\n",
        "    u\"DX\":\"Great dismay\",\n",
        "    u\":‑O\":\"Surprise\",\n",
        "    u\":O\":\"Surprise\",\n",
        "    u\":‑o\":\"Surprise\",\n",
        "    u\":o\":\"Surprise\",\n",
        "    u\":-0\":\"Shock\",\n",
        "    u\"8‑0\":\"Yawn\",\n",
        "    u\">:O\":\"Yawn\",\n",
        "    u\":-\\*\":\"Kiss\",\n",
        "    u\":\\*\":\"Kiss\",\n",
        "    u\":X\":\"Kiss\",\n",
        "    u\";‑\\)\":\"Wink or smirk\",\n",
        "    u\";\\)\":\"Wink or smirk\",\n",
        "    u\"\\*-\\)\":\"Wink or smirk\",\n",
        "    u\"\\*\\)\":\"Wink or smirk\",\n",
        "    u\";‑\\]\":\"Wink or smirk\",\n",
        "    u\";\\]\":\"Wink or smirk\",\n",
        "    u\";\\^\\)\":\"Wink or smirk\",\n",
        "    u\":‑,\":\"Wink or smirk\",\n",
        "    u\";D\":\"Wink or smirk\",\n",
        "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":‑\\|\":\"Straight face\",\n",
        "    u\":\\|\":\"Straight face\",\n",
        "    u\":$\":\"Embarrassed or blushing\",\n",
        "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\"O:‑\\)\":\"Angel, saint or innocent\",\n",
        "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
        "    u\"0:3\":\"Angel, saint or innocent\",\n",
        "    u\"0:‑\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
        "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
        "    u\">:‑\\)\":\"Evil or devilish\",\n",
        "    u\">:\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:‑\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:\\)\":\"Evil or devilish\",\n",
        "    u\"3:‑\\)\":\"Evil or devilish\",\n",
        "    u\"3:\\)\":\"Evil or devilish\",\n",
        "    u\">;\\)\":\"Evil or devilish\",\n",
        "    u\"\\|;‑\\)\":\"Cool\",\n",
        "    u\"\\|‑O\":\"Bored\",\n",
        "    u\":‑J\":\"Tongue-in-cheek\",\n",
        "    u\"#‑\\)\":\"Party all night\",\n",
        "    u\"%‑\\)\":\"Drunk or confused\",\n",
        "    u\"%\\)\":\"Drunk or confused\",\n",
        "    u\":-###..\":\"Being sick\",\n",
        "    u\":###..\":\"Being sick\",\n",
        "    u\"<:‑\\|\":\"Dump\",\n",
        "    u\"\\(>_<\\)\":\"Troubled\",\n",
        "    u\"\\(>_<\\)>\":\"Troubled\",\n",
        "    u\"\\(';'\\)\":\"Baby\",\n",
        "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
        "    u\"\\(\\^_-\\)\":\"Wink\",\n",
        "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
        "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
        "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
        "    u\"\\^_\\^\":\"Joyful\",\n",
        "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
        "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
        "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
        "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
        "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;_;\":\"Sad of Crying\",\n",
        "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
        "    u\";_;\":\"Sad or Crying\",\n",
        "    u\";-;\":\"Sad or Crying\",\n",
        "    u\";n;\":\"Sad or Crying\",\n",
        "    u\";;\":\"Sad or Crying\",\n",
        "    u\"Q\\.Q\":\"Sad or Crying\",\n",
        "    u\"T\\.T\":\"Sad or Crying\",\n",
        "    u\"QQ\":\"Sad or Crying\",\n",
        "    u\"Q_Q\":\"Sad or Crying\",\n",
        "    u\"\\(-\\.-\\)\":\"Shame\",\n",
        "    u\"\\(-_-\\)\":\"Shame\",\n",
        "    u\"\\(一一\\)\":\"Shame\",\n",
        "    u\"\\(；一_一\\)\":\"Shame\",\n",
        "    u\"\\(=_=\\)\":\"Tired\",\n",
        "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
        "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
        "    u\"=_\\^=\t\":\"cat\",\n",
        "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
        "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
        "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
        "    u\"\\(\\・\\・?\":\"Confusion\",\n",
        "    u\"\\(?_?\\)\":\"Confusion\",\n",
        "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
        "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
        "    u\"\\^/\\^\":\"Normal Laugh\",\n",
        "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
        "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
        "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
        "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
        "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
        "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
        "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
        "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
        "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
        "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
        "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
        "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
        "    u'\\(-\"-\\)':\"Worried\",\n",
        "    u\"\\(ーー;\\)\":\"Worried\",\n",
        "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
        "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
        "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
        "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
        "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
        "    u\":O o_O\":\"Surprised\",\n",
        "    u\"o_0\":\"Surprised\",\n",
        "    u\"o\\.O\":\"Surpised\",\n",
        "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
        "    u\"oO\":\"Surprised\",\n",
        "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
        "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n",
        "}\n",
        "\n",
        "\n",
        "def emoticon_to_word(text):\n",
        "    for emot in EMOTICONS:\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
        "        text = text.replace('_', ' ')\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2JKfjdvbfyC"
      },
      "source": [
        "# emojis to words\n",
        "##\n",
        "!pip install demoji\n",
        "\n",
        "import demoji\n",
        "demoji.download_codes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f60GEDjEbi-B"
      },
      "source": [
        "# convert emojis to words\n",
        "##\n",
        "def emoji_to_word(text):\n",
        "  emojis = demoji.findall(text)\n",
        "  for emoji in emojis.keys():\n",
        "    text = text.replace(emoji, emojis.get(emoji))\n",
        "  return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6TiOgpRbo7I"
      },
      "source": [
        "# lowercase\n",
        "read_file['clean_text'] = read_file['content'].apply(str.lower)\n",
        "\n",
        "#remove html\n",
        "read_file['clean_text'] = read_file['clean_text'].apply(html)\n",
        "\n",
        "#remove mentions\n",
        "read_file['clean_text'] = read_file['clean_text'].apply(remove_mention)\n",
        "\n",
        "# remove urls\n",
        "##\n",
        "read_file['clean_text'] = read_file['clean_text'].apply(remove_url)\n",
        "\n",
        "# remove numbers\n",
        "##\n",
        "read_file['clean_text'] = read_file['clean_text'].apply(remove_numbers)\n",
        "\n",
        "# remove punctuations\n",
        "##\n",
        "read_file['clean_text'] = read_file['clean_text'].apply(remove_punct)\n",
        "\n",
        "# remove stopwords\n",
        "##\n",
        "read_file['clean_text'] = read_file['clean_text'].apply(remove_stopword)\n",
        "\n",
        "# convert emoticons to words\n",
        "##\n",
        "read_file['clean_text'] = read_file['clean_text'].apply(emoticon_to_word)\n",
        "\n",
        "# convert emojis to words\n",
        "##\n",
        "read_file['clean_text'] = read_file['clean_text'].apply(emoji_to_word)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLpfx1Dqcctz"
      },
      "source": [
        "# lemmatization\n",
        "##\n",
        "read_file['text_preprocessed'] = read_file['clean_text'].apply(do_lemmatize)\n",
        "\n",
        "\n",
        "# remove short words\n",
        "read_file['text_preprocessed'] = read_file['text_preprocessed'].apply(lambda x: ' '.join([w for w in str(x).split() if len(w)>3]))\n",
        "\n",
        "\n",
        "read_file['tokenized_tweet'] = read_file.apply(lambda row: nltk.word_tokenize(row['text_preprocessed']), axis=1)\n",
        "\n",
        "\n",
        "#read_file['text_preprocessed'] = tokenized_tweet\n",
        "\n",
        "read_file.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_V74gKTIaBJ"
      },
      "source": [
        "pos = read_file['text_preprocessed']\n",
        "print(pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfJJnredIZ-r"
      },
      "source": [
        "#Stopwords\n",
        "stop = stopwords.words('english')\n",
        "print(stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScdegR8sIZ75"
      },
      "source": [
        "# POS tagger dictionary\n",
        "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
        "def token_stop_pos(pos):\n",
        "    tags = pos_tag(pos)\n",
        "    newlist = []\n",
        "    for word, tag in tags:\n",
        "        if word.lower() not in set(stopwords.words('english')):\n",
        "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
        "    return newlist\n",
        "\n",
        "read_file['POS tagged'] = pos.apply(token_stop_pos)\n",
        "read_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mrKuFUAjWk1"
      },
      "source": [
        "read_file = read_file.drop(['POS tagged'],axis=1)\n",
        "read_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEaq2tuT9_xj"
      },
      "source": [
        "# Medical Words\n",
        "nlp = spacy.load('en_core_sci_md')\n",
        "def disease_words(disease):\n",
        "    disease1 = nlp(disease)\n",
        "    disease2 = [w.text for w in disease1.ents]\n",
        "    return disease2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbr8S5qQ-qKc"
      },
      "source": [
        "read_file['Medical Words'] = read_file['Lemma'].apply(disease_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmVHA33-kXlD"
      },
      "source": [
        "read_file.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6014gBiWMpdc"
      },
      "source": [
        "# EXPLORATORY DATA ANALYSIS\n",
        "\n",
        "Let's do a few visualizations and NLP operations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzWJ9X_YAJVb"
      },
      "source": [
        "# getting the date column ready for datetime operations\n",
        "read_file['date']= pd.to_datetime(read_file['date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZbYVYJdsVzX"
      },
      "source": [
        "#Data Visualisation\n",
        "content_df = read_file['content']\n",
        "print(len(content_df))\n",
        "content_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9HRD48KsVi4"
      },
      "source": [
        "tweet_len = []\n",
        "for contentRow in content_df:\n",
        "    lengthofTweet = len(contentRow.split())\n",
        "    tweet_len.append(lengthofTweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSmEcVxLsyEG"
      },
      "source": [
        "read_file.insert(2,\"Word Length of Tweet\", tweet_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3mQAJSJsxzq"
      },
      "source": [
        "print('Average number of words used per tweet:', round(read_file['Word Length of Tweet'].mean(),2))\n",
        "print('Spread of number of words used per tweet:',round(read_file['Word Length of Tweet'].std(),2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYmn_AWctN4j"
      },
      "source": [
        "character_tweet_len = []\n",
        "for contentRow in content_df:\n",
        "    lengthofTweet = len(contentRow.split())\n",
        "    character_tweet_len.append(lengthofTweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8t8vgEtN06"
      },
      "source": [
        "read_file.insert(2,\"Character Length of Tweet\", character_tweet_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLXb4f-htNyJ"
      },
      "source": [
        "print('Average number of words used per tweet:', round(read_file['Character Length of Tweet'].mean(),2))\n",
        "print('Spread of number of words used per tweet:',round(read_file['Character Length of Tweet'].std(),2))\n",
        "print('Minimum number of number of words used per tweet:',read_file['Character Length of Tweet'].min())\n",
        "print('Maximum number of words used per tweet:',read_file['Character Length of Tweet'].max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5fm-wgmxySR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(read_file['Word Length of Tweet'], bins=50)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Word Length of Tweet');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w8cgqzrNZmy"
      },
      "source": [
        "source_L = read_file['sourceLabel'].value_counts(ascending=True)\n",
        "source_num = source_L.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UclKzOkxNZjW"
      },
      "source": [
        "source_num.plot.pie(subplots=True, figsize=(15, 10),autopct='%1.0f%%',title=\"Source of Tweets\")\n",
        "plt.legend(loc=\"lower right\", fontsize=10,prop={'size': 12})\n",
        "print(\"Other sources = 1885\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCcZ3UI6NZhx"
      },
      "source": [
        "df_1try = read_file.groupby('date')[[\"Character Length of Tweet\",\"Word Length of Tweet\"]].mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il7W3BcxNZe1"
      },
      "source": [
        "read_file.groupby('sourceLabel')[[\"Word Length of Tweet\"]].mean()\\\n",
        ".plot(kind='line', figsize=(20,10))\\\n",
        ".legend(loc='center left', bbox_to_anchor=(1, 0.5));  # set legend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq4iFWHV1SYD"
      },
      "source": [
        "## The most used sources for tweeting about stress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7C4-yUr1G5A"
      },
      "source": [
        "def plot_frequency_charts(df, user, content):\n",
        "    read_file = pd.DataFrame()\n",
        "    read_file[user] = df[user]\n",
        "    \n",
        "    f, ax = plt.subplots(1,1, figsize=(16,4))\n",
        "    total = float(len(df))\n",
        "    g = sns.countplot(df[user], order = df[user].value_counts().index[:20])\n",
        "    g.set_title(\"Number and percentage of {}\".format(content))\n",
        "\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        ax.text(p.get_x()+p.get_width()/2.,\n",
        "                height + 3,\n",
        "                '{:1.2f}%'.format(100*height/total),\n",
        "                ha=\"center\") \n",
        "\n",
        "    plt.title('Frequency of {} tweeting about Corona'.format(user))\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.xlabel(content, fontsize=12)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzQvELxs1Mmi"
      },
      "source": [
        "plot_frequency_charts(read_file, 'sourceLabel','Source')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zFADUFC1gNN"
      },
      "source": [
        "user = read_file['user']\n",
        "user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k7oEcTHDS_z"
      },
      "source": [
        "# A function to count tweets based on regular expressions\n",
        "def count_tweets(reg_expression, tweet):\n",
        "    tweets_list = re.findall(reg_expression, tweet)\n",
        "    return len(tweets_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyoXMIC_DS81"
      },
      "source": [
        "# Creating a dictionary to hold these counts\n",
        "content_count = {\n",
        "    'words' : read_file['content'].apply(lambda x: count_tweets(r'\\w+', x)),\n",
        "    'mentions' : read_file['content'].apply(lambda x: count_tweets(r'@\\w+', x)),\n",
        "    'hashtags' : read_file['content'].apply(lambda x: count_tweets(r'#\\w+', x)),\n",
        "    'urls' : read_file['content'].apply(lambda x: count_tweets(r'http.?://[^\\s]+[\\s]?', x)),   \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1TbvGpYDS12"
      },
      "source": [
        "read_file = pd.concat([read_file, pd.DataFrame(content_count)], axis=1)\n",
        "read_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1Dig-YjYd5"
      },
      "source": [
        "# SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wAZkEmF9QSK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fXe9LZ8Jotu"
      },
      "source": [
        "#Subjectivity\n",
        "def getsubjectivity(text):\n",
        "    return TextBlob(text).sentiment.subjectivity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nss63k0uJoq-"
      },
      "source": [
        "#Polarity to know how positive or negative the text is\n",
        "def getpolarity(text):\n",
        "     return TextBlob(text).sentiment.polarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRAkzVh3J02b"
      },
      "source": [
        "#Create 2 new columns \n",
        "\n",
        "#read_file.insert(7,\"Subjectivity\", clean_file.apply(getsubjectivity))\n",
        "#read_file.insert(8,\"Polarity\", clean_file.apply(getpolarity))\n",
        "\n",
        "read_file['Subjectivity'] = read_file['text_preprocessed'].apply(getsubjectivity)\n",
        "read_file['Polarity'] = read_file['text_preprocessed'].apply(getpolarity)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q99esV9VJ0zA"
      },
      "source": [
        "#Compue negative, neutral and positive analysis\n",
        "def analysis(points):\n",
        "    if points < 0:\n",
        "        return 'Negative'\n",
        "    elif points == 0:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Positive'\n",
        "read_file['SentimentAnalysis'] = read_file['Polarity'].apply(analysis)\n",
        "read_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH2qZlY9J0tp"
      },
      "source": [
        "#All the positive Tweets\n",
        "i = 1\n",
        "sorted_data = read_file.sort_values(by=['Polarity'])\n",
        "positive = []\n",
        "for j in range(0, sorted_data.shape[0]):\n",
        "    if (sorted_data['SentimentAnalysis'][j] == 'Positive'):\n",
        "        positive += str(i)\n",
        "        print(str(i) + ')' + sorted_data['content'][j])\n",
        "        print()\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDrESFdzJ0rO"
      },
      "source": [
        "#All the negative Tweets\n",
        "i = 1\n",
        "sorted_data = read_file.sort_values(by=['Polarity'], ascending='False')\n",
        "for j in range(0, sorted_data.shape[0]):\n",
        "    if (sorted_data['SentimentAnalysis'][j] == 'Negative'):\n",
        "        \n",
        "        print(str(i) + ')' + sorted_data['content'][j])\n",
        "        print()\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p4yZaV4KqPu"
      },
      "source": [
        "read_file['SentimentAnalysis'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4RDOwQGJ0oG"
      },
      "source": [
        "# Plot Subjectivity and Polarity\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for i in range(0, read_file.shape[0]):\n",
        "    plt.scatter(read_file['Polarity'][i], read_file['Subjectivity'][i], color='violet')\n",
        "    \n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Subjectivity')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw5DzpmvKHYR"
      },
      "source": [
        "#Positive tweets percentage\n",
        "positive_tweets = read_file[read_file.SentimentAnalysis == 'Positive']\n",
        "positive_tweets = positive_tweets['content']\n",
        "\n",
        "round((positive_tweets.shape[0] / read_file.shape[0])* 100, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To42t8FbKHU4"
      },
      "source": [
        "#Negative tweets percentage\n",
        "negative_tweets = read_file[read_file.SentimentAnalysis == 'Negative']\n",
        "negative_tweets = negative_tweets['content']\n",
        "\n",
        "round((negative_tweets.shape[0] / read_file.shape[0])* 100, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbrQAhQjyL2-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZzqKFpRKHSX"
      },
      "source": [
        "#Visualize the count\n",
        "\n",
        "read_file['SentimentAnalysis'].value_counts()\n",
        "\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Counts')\n",
        "read_file['SentimentAnalysis'].value_counts().plot(kind='bar')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlw9ArO2KHQN"
      },
      "source": [
        "#Word Cloud\n",
        "words = ' '.join([tweets for tweets in read_file['text_preprocessed']])\n",
        "Cloud = WordCloud(background_color = 'white', max_words=500, colormap='Set2', width = 3000, height = 2000, random_state = 99, max_font_size = 500).generate(words)\n",
        "\n",
        "plt.imshow(Cloud, interpolation = \"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPoN6WaaKHNV"
      },
      "source": [
        "# Generate a word cloud image\n",
        "stopwords = set(STOPWORDS)\n",
        "mask = np.array(Image.open(\"/content/drive/MyDrive/Colab Notebooks/BIA/m.png\"))\n",
        "wordcloud = WordCloud(stopwords=stopwords,width = 3000, height = 2000, background_color=\"white\", max_words=1000, mask=mask).generate(' '.join(clean_file['Lemma']))\n",
        "# create twitter image\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "# store to file\n",
        "plt.savefig(\"twitter.png\", format=\"png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9qUkMfQbM16q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C8eOFxbrIDh"
      },
      "source": [
        "# Binary classification using SVM & Logistic Regression algorithms to classify if a tweet is made from a bot or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-SDxEGcAmcs"
      },
      "source": [
        "svm_read_file = read_file\n",
        "svm_read_file = svm_read_file .drop([\"url\",\"date\",\"Character Length of Tweet\",\"Word Length of Tweet\",\"renderedContent\", \"id\",\"user\", \"replyCount\", \"retweetCount\"], axis=1)\n",
        "svm_read_file = svm_read_file.drop([\"likeCount\",\"conversationId\",\"lang\",\"coordinates\",\"place\",\"text_preprocessed\",\"clean_text\"], axis=1)\n",
        "svm_read_file = svm_read_file.drop([\"words\",\"mentions\",\"hashtags\",\"urls\",\"Subjectivity\", \"Polarity\", \"SentimentAnalysis\",\"tp_a\"],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V8beg4nAuNa"
      },
      "source": [
        "svm_read_file[\"sourceLabel\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzEeOLqNAuUV"
      },
      "source": [
        "## If binary source not in dataframe\n",
        "l = []\n",
        "for source in svm_read_file.sourceLabel:\n",
        "    if source == \"TweetDeck\" or source == \"Twitter for iPad\" or source == \"Twitter Web App\" or source == \"Twitter for Android\" or source == \"Twitter for iPhone\":\n",
        "        l.append(0)\n",
        "    else:\n",
        "        l.append(1)\n",
        "svm_read_file.insert(2, \"binarySource\", l, True )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UCJh_6GAuZS"
      },
      "source": [
        "X = svm_read_file[\"content\"]\n",
        "y = svm_read_file[\"binarySource\"]\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "tfidf_vect = TfidfVectorizer(stop_words=None, min_df=1) \n",
        "X_train= tfidf_vect.fit_transform(X_train)\n",
        "X_test = tfidf_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0PqX_Y4AucD"
      },
      "source": [
        "sm = SMOTE(random_state=3)\n",
        "x_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Before SMOTE :\" , Counter(y_train))\n",
        "print(\"After SMOTE :\" , Counter(y_train_smote))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2mpF8DeAulL"
      },
      "source": [
        "clf = svm.LinearSVC(C=1.0).fit(x_train_smote, y_train_smote)\n",
        "y_pred=clf.predict(X_test)\n",
        "labels=sorted(test_y.unique())\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test.values, y_pred))\n",
        "precision, recall, fscore, support=precision_recall_fscore_support(y_test.values, y_pred, labels=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_scores = clf.decision_function(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test.values, decision_scores, pos_label=1)\n",
        "auc_score = round(auc(fpr, tpr)*100,2)\n",
        "prc_score = round(average_precision_score(y_test.values, decision_scores) *100,2)\n",
        "\n",
        "print(\"AUC: \", auc_score,\"%\", \"PRC: \", prc_score,\"%\")\n",
        "plt.figure();\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2);\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')    \n",
        "plt.xlim([0.0, 1.0]);\n",
        "plt.ylim([0.0, 1.05]);\n",
        "plt.xlabel('False Positive Rate');\n",
        "plt.ylabel('True Positive Rate');\n",
        "plt.title('SVM-AUC');\n",
        "plt.show();\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test.values, decision_scores, pos_label=1)\n",
        "\n",
        "plt.figure();\n",
        "plt.plot(recall, precision, color='darkorange', lw=2);\n",
        "plt.xlim([0.0, 1.0]);\n",
        "plt.ylim([0.0, 1.05]);\n",
        "plt.xlabel('Recall');\n",
        "plt.ylabel('Precision');\n",
        "plt.title('SVM-PRC');\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "shhT0RTfNUv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q967oCQKKwQQ"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf2 = LogisticRegression().fit(x_train_smote, y_train_smote)\n",
        "y_pred=clf2.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test.values, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_scores = clf2.decision_function(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test.values, decision_scores, pos_label=1)\n",
        "auc_score = round(auc(fpr, tpr)*100,2)\n",
        "prc_score = round(average_precision_score(y_test.values, decision_scores) *100,2)\n",
        "\n",
        "print(\"AUC: \", auc_score,\"%\", \"PRC: \", prc_score,\"%\")\n",
        "plt.figure();\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2);\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')    \n",
        "plt.xlim([0.0, 1.0]);\n",
        "plt.ylim([0.0, 1.05]);\n",
        "plt.xlabel('False Positive Rate');\n",
        "plt.ylabel('True Positive Rate');\n",
        "plt.title('SVM-AUC');\n",
        "plt.show();\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test.values, decision_scores, pos_label=1)\n",
        "\n",
        "plt.figure();\n",
        "plt.plot(recall, precision, color='darkorange', lw=2);\n",
        "plt.xlim([0.0, 1.0]);\n",
        "plt.ylim([0.0, 1.05]);\n",
        "plt.xlabel('Recall');\n",
        "plt.ylabel('Precision');\n",
        "plt.title('SVM-PRC');\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "GKxfc4IiNWNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v7yitKbmL9G"
      },
      "source": [
        "# TOPIC MODELLING - LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg8r5V7HXIaO"
      },
      "source": [
        "print(read_file.columns)\n",
        "print(read_file.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrUNdob8XIXh"
      },
      "source": [
        "read_file['text_preprocessed'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwMan_0B3mda"
      },
      "source": [
        "stop = stopwords.words('english')\n",
        "print(stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlSaBE4iYQD7"
      },
      "source": [
        "def tokenize(text):\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_tokens = [w for w in word_tokens if not w in stop_words if len(w) > 2]\n",
        "    return filtered_tokens\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize, use_idf=True,smooth_idf=True)\n",
        "\n",
        "svd_model = TruncatedSVD(n_components=10, algorithm='randomized',n_iter=10)\n",
        "\n",
        "svd_transformer = Pipeline([('tfidf', vectorizer), ('svd', svd_model)])\n",
        "\n",
        "svd_matrix = svd_transformer.fit_transform(read_file.text_preprocessed)\n",
        "\n",
        "tfidf = svd_transformer.steps[0][-1]\n",
        "voc = tfidf.get_feature_names()\n",
        "\n",
        "features_names = np.array(voc)\n",
        "\n",
        "sorting = np.argsort(svd_model.components_, axis=1)[:, ::-1]\n",
        "\n",
        "mglearn.tools.print_topics(topics=range(10), feature_names=features_names,\n",
        "                           sorting=sorting, topics_per_chunk=5, n_words=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKwBk7R-mhNh"
      },
      "source": [
        "# EMOTION ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWP-xW06YP0f"
      },
      "source": [
        "read_file['emotion'] = read_file.apply(lambda row: te.get_emotion(row['text_preprocessed']), axis=1)\n",
        "read_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGhBA6UeYPxz"
      },
      "source": [
        "read_file['emotion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_1MVw-ERDgG"
      },
      "source": [
        "read_file['Happy']=[dict(i)['Happy'] for i in read_file['emotion']]\n",
        "read_file['Angry']=[dict(i)['Angry'] for i in read_file['emotion']]\n",
        "read_file['Surprise']=[dict(i)['Surprise'] for i in read_file['emotion']]\n",
        "read_file['Sad']=[dict(i)['Sad'] for i in read_file['emotion']]\n",
        "read_file['Fear']=[dict(i)['Fear'] for i in read_file['emotion']]\n",
        "dominant=[]\n",
        "for i in read_file['emotion']:\n",
        "    p=dict(i)\n",
        "    Keymax = max(p, key=p.get)\n",
        "    dominant.append(Keymax)\n",
        "read_file['dominant emotion'] = dominant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqK8P6aCRDaC"
      },
      "source": [
        "read_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NMd70q9SkX_"
      },
      "source": [
        "#Visualize the count\n",
        "\n",
        "read_file['dominant emotion'].value_counts()\n",
        "\n",
        "plt.title('Emotion Analysis')\n",
        "plt.xlabel('Emotion')\n",
        "plt.ylabel('Counts')\n",
        "read_file['dominant emotion'].value_counts().plot(kind='bar')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bfhFzsP4BNL"
      },
      "source": [
        "dd=read_file[read_file['dominant emotion']=='Happy']\n",
        "text=\" \".join(dd['text_preprocessed'])\n",
        "text=text.replace(\"stock\",' ')\n",
        "text=text.replace(\"GME\",\" \")\n",
        "text=text.replace(\"nan\",' ')\n",
        "wordcloud = WordCloud(width=1500, height=500).generate(text)\n",
        "plt.figure(figsize=(18,8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear');\n",
        "plt.title('Happy') ;\n",
        "plt.axis(\"off\") ;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejCMy8wB3KKu"
      },
      "source": [
        "# Wordplot for being angry,sad,emotional\n",
        "fig, ax = plt.subplots(2, 2, figsize=(16, 8))\n",
        "k=0\n",
        "j=0\n",
        "for i in ['Sad','Fear','Surprise','Angry']:\n",
        "    dd=read_file[read_file['dominant emotion']==i]\n",
        "    text=\" \".join(dd['text_preprocessed'])\n",
        "    text=text.replace(\"stock\",' ')\n",
        "    text=text.replace(\"GME\",\" \")\n",
        "    text=text.replace(\"nan\",' ')\n",
        "    wordcloud = WordCloud(width=1500, height=500).generate(text)\n",
        "\n",
        "    ax[k,j].imshow(wordcloud, interpolation='bilinear')\n",
        "    ax[k,j].set_title(i)\n",
        "    ax[k, j].set_axis_off()\n",
        "    j+=1\n",
        "    if j>1:\n",
        "        k+=1\n",
        "        j=0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}